#!/usr/bin/env python3
"""
TTS Trainer - Pipeline-Orchestrated Architecture
Main CLI entry point for all operations
"""

import argparse
import asyncio
import sys
from pathlib import Path
from typing import Optional
import click
import json
import shutil

# Add src to path for imports
sys.path.insert(0, str(Path(__file__).parent / "src"))

from pipeline.orchestrator import PipelineOrchestrator
from models.base_trainer import ModelRegistry
from utils.logging_utils import setup_logging
from utils.file_utils import validate_input_path, create_output_dir


def setup_parser() -> argparse.ArgumentParser:
    """Setup the main argument parser with all subcommands."""
    parser = argparse.ArgumentParser(
        description="TTS Trainer - Convert videos to TTS training data and train models",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Run full pipeline from videos to trained model
  python main.py run-pipeline --input resources/videos/ --output artifacts/models/
  
  # Extract audio from videos only
  python main.py extract-audio --input resources/videos/ --output resources/audio/
  
  # Train a specific model
  python main.py train --model xtts_v2 --dataset resources/datasets/my_voice/
  
  # Launch Discord bot
  python main.py discord-bot --token YOUR_DISCORD_TOKEN
  
  # Test inference
  python main.py inference --model artifacts/models/my_model.pth --text "Hello world!"
        """)
    
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose logging")
    parser.add_argument("--config", type=str, help="Path to custom config file")
    
    subparsers = parser.add_subparsers(dest="command", help="Available commands")
    
    # Run Pipeline Command
    pipeline_parser = subparsers.add_parser("run-pipeline", help="Run the full video-to-model pipeline")
    pipeline_parser.add_argument("--input", required=True, help="Input directory containing video files")
    pipeline_parser.add_argument("--output", required=True, help="Output directory for trained models")
    pipeline_parser.add_argument("--model", default="xtts_v2", choices=["xtts_v2", "vits", "tortoise"], 
                                 help="Model type to train")
    pipeline_parser.add_argument("--skip-training", action="store_true", 
                                 help="Skip training, only prepare dataset")
    
    # Extract Audio Command
    audio_parser = subparsers.add_parser("extract-audio", help="Extract audio from video files")
    audio_parser.add_argument("--input", required=True, help="Input directory containing video files")
    audio_parser.add_argument("--output", default="resources/audio/", help="Output directory for audio files")
    audio_parser.add_argument("--format", default="wav", choices=["wav", "mp3", "flac"], help="Output audio format")
    
    # Preprocess Audio Command
    preprocess_parser = subparsers.add_parser("preprocess-audio", help="Preprocess audio files for TTS training")
    preprocess_parser.add_argument("--input", required=True, help="Input directory containing audio files")
    preprocess_parser.add_argument("--output", default="resources/audio/processed/", help="Output directory for processed audio")
    preprocess_parser.add_argument("--config", help="Path to audio preprocessing config file")
    preprocess_parser.add_argument("--validate-only", action="store_true", help="Only validate quality, don't process")
    
    # Transcribe Command
    transcribe_parser = subparsers.add_parser("transcribe", help="Generate transcripts from audio files")
    transcribe_parser.add_argument("--input", required=True, help="Input directory containing audio files")
    transcribe_parser.add_argument("--output", default="resources/transcripts/", help="Output directory for transcripts")
    transcribe_parser.add_argument("--model", default="large-v3", help="Whisper model size")
    transcribe_parser.add_argument("--speaker-diarization", action="store_true", help="Enable speaker separation")
    transcribe_parser.add_argument("--device", default="auto", help="Device for processing (auto, cpu, cuda)")
    transcribe_parser.add_argument("--diarization-config", help="Path to speaker diarization config file")
    
    # Speaker Segmentation Command
    segment_parser = subparsers.add_parser("segment-speakers", help="Extract character-specific audio clips")
    segment_parser.add_argument("--audio", required=True, help="Input directory containing original audio files")
    segment_parser.add_argument("--transcripts", required=True, help="Directory containing transcription results")
    segment_parser.add_argument("--output", default="resources/segments/", help="Output directory for character segments")
    segment_parser.add_argument("--config", help="Path to speaker mapping config file")
    segment_parser.add_argument("--analyze-only", action="store_true", help="Only analyze speakers, don't extract segments")
    
    # Speaker Analysis Command
    analyze_parser = subparsers.add_parser("analyze-speakers", help="Analyze speaker patterns in transcripts")
    analyze_parser.add_argument("--transcripts", required=True, help="Directory containing transcription results")
    analyze_parser.add_argument("--output", help="Output file for analysis report")
    
    # Process Segments Command
    process_segments_parser = subparsers.add_parser("process-segments", help="Improve quality of audio segments for TTS training")
    process_segments_parser.add_argument("--input", required=True, help="Input directory containing speaker segments")
    process_segments_parser.add_argument("--output", default="resources/segments_processed/", help="Output directory for processed segments")
    process_segments_parser.add_argument("--config", help="Path to segment processing config file")
    
    # Voice Clustering Command
    clustering_parser = subparsers.add_parser("cluster-voices", help="Group similar voices across episodes for consistent speaker IDs")
    clustering_parser.add_argument("--segments", required=True, help="Input directory containing speaker segments")
    clustering_parser.add_argument("--output", default="resources/voice_clusters/", help="Output directory for clustering results")
    clustering_parser.add_argument("--config", help="Path to voice clustering config file")
    clustering_parser.add_argument("--apply", help="Apply existing clustering results to reorganize segments by character")
    
    # Train Command
    train_parser = subparsers.add_parser("train", help="Train a TTS model")
    train_parser.add_argument("--model", required=True, choices=["xtts_v2", "vits", "tortoise"],
                              help="Model type to train")
    train_parser.add_argument("--dataset", required=True, help="Path to training dataset")
    train_parser.add_argument("--output", default="artifacts/models/", help="Output directory for trained model")
    train_parser.add_argument("--epochs", type=int, help="Number of training epochs (overrides config)")
    train_parser.add_argument("--batch-size", type=int, help="Batch size (overrides config)")
    train_parser.add_argument("--resume", help="Path to checkpoint to resume from")
    
    # Inference Command
    inference_parser = subparsers.add_parser("inference", help="Test model inference")
    inference_parser.add_argument("--model", required=True, help="Path to trained model or model name")
    inference_parser.add_argument("--text", required=True, help="Text to synthesize")
    inference_parser.add_argument("--reference", help="Path to reference audio file for voice cloning")
    inference_parser.add_argument("--output", default="inference_output.wav", help="Output audio file")
    inference_parser.add_argument("--streaming", action="store_true", help="Enable streaming inference")
    
    # Discord Bot Command
    bot_parser = subparsers.add_parser("discord-bot", help="Launch Discord bot")
    bot_parser.add_argument("--token", required=True, help="Discord bot token")
    bot_parser.add_argument("--model", default="xtts_v2", help="Model to use for TTS")
    bot_parser.add_argument("--voice-clone", help="Path to reference audio for voice cloning")
    
    # Validate Command
    validate_parser = subparsers.add_parser("validate", help="Validate audio/dataset quality")
    validate_parser.add_argument("--input", required=True, help="Input directory to validate")
    validate_parser.add_argument("--type", choices=["audio", "dataset", "transcripts"], 
                                 default="audio", help="Type of validation to perform")
    
    # Create Validation Samples Command
    validation_samples_parser = subparsers.add_parser("create-validation-samples", 
                                                       help="Create validation samples for manual review of clustering quality")
    validation_samples_parser.add_argument("--clustering-results", required=True, 
                                          help="Path to voice clustering results JSON file")
    validation_samples_parser.add_argument("--output", required=True, 
                                          help="Output directory for validation samples")
    validation_samples_parser.add_argument("--samples-per-cluster", type=int, default=3, 
                                          help="Number of samples per cluster to copy")
    
    return parser


async def run_pipeline(args):
    """Run the full video-to-model pipeline."""
    print(f"ğŸš€ Starting full pipeline: {args.input} â†’ {args.output}")
    
    orchestrator = PipelineOrchestrator(
        model_type=args.model,
        skip_training=args.skip_training,
        config_path=args.config
    )
    
    result = await orchestrator.run_full_pipeline(
        input_path=args.input,
        output_path=args.output
    )
    
    if result.success:
        print(f"âœ… Pipeline completed successfully!")
        print(f"ğŸ“ Model saved to: {result.model_path}")
        print(f"ğŸ“Š Training metrics: {result.metrics}")
    else:
        print(f"âŒ Pipeline failed: {result.error}")
        return 1
    
    return 0


async def extract_audio(args):
    """Extract audio from video files."""
    print(f"ğŸµ Extracting audio: {args.input} â†’ {args.output}")
    
    from pipeline.stages.audio_extractor import AudioExtractor
    
    extractor = AudioExtractor(output_format=args.format)
    result = await extractor.extract_from_directory(args.input, args.output)
    
    if result['success']:
        print(f"âœ… Extracted {result['files_successful']} audio files successfully")
        if result['files_failed'] > 0:
            print(f"âš ï¸ Failed to extract {result['files_failed']} files")
            if result.get('errors'):
                print("Errors:")
                for error in result['errors'][:5]:  # Show first 5 errors
                    print(f"   - {error}")
    else:
        print(f"âŒ Audio extraction failed: {result.get('error', 'Unknown error')}")
        return 1
    
    return 0


async def preprocess_audio(args):
    """Preprocess audio files for TTS training."""
    print(f"ğŸ”§ Preprocessing audio: {args.input} â†’ {args.output}")
    
    from pipeline.stages.audio_preprocessor import AudioPreprocessor
    from pipeline.validators.audio_quality import AudioQualityValidator
    
    if args.validate_only:
        # Only validate audio quality
        validator = AudioQualityValidator(args.config) if args.config else AudioQualityValidator()
        result = await validator.validate_directory(args.input)
        
        print(f"ğŸ“Š Audio Quality Report:")
        print(f"   Overall Score: {result.overall_score:.1f}/10")
        print(f"   Passed Files: {result.passed_files}")
        print(f"   Failed Files: {result.failed_files}")
        
        if result.issues:
            print(f"   Issues found:")
            for issue in result.issues[:10]:  # Show first 10 issues
                print(f"      - {issue}")
        
        if result.overall_score >= 7.0:
            print("âœ… Audio quality is acceptable for TTS training")
        else:
            print("âš ï¸ Audio quality may need improvement")
            
        return 0 if result.overall_score >= 5.0 else 1
    
    else:
        # Process audio files
        preprocessor = AudioPreprocessor(args.config) if args.config else AudioPreprocessor()
        result = await preprocessor.process_directory(args.input, args.output)
        
        if result['success']:
            print(f"âœ… Processed {result['files_processed']} audio files successfully")
            if result.get('files_failed', 0) > 0:
                print(f"âš ï¸ Failed to process {result['files_failed']} files")
                if result.get('errors'):
                    print("Errors:")
                    for error in result['errors'][:5]:  # Show first 5 errors
                        print(f"   - {error}")
            
            validation_score = result.get('validation_score', 0)
            print(f"ğŸ“Š Processed audio quality score: {validation_score:.1f}/10")
            
            if validation_score >= 7.0:
                print("âœ… Processed audio quality is good for TTS training")
            else:
                print("âš ï¸ Some processed audio may need attention")
                
        else:
            print(f"âŒ Audio preprocessing failed: {result.get('error', 'Unknown error')}")
            return 1
        
        return 0


async def transcribe_audio(args):
    """Generate transcripts from audio files."""
    print(f"ğŸ“ Transcribing audio: {args.input} â†’ {args.output}")
    
    from pipeline.stages.transcriber import Transcriber
    
    transcriber = Transcriber(
        model_size=args.model,
        speaker_diarization=args.speaker_diarization,
        device=args.device,
        diarization_config=args.diarization_config
    )
    result = await transcriber.process_directory(args.input, args.output)
    
    if result['success']:
        print(f"âœ… Processed {result['files_processed']} audio files successfully")
        if result.get('files_failed', 0) > 0:
            print(f"âš ï¸ Failed to process {result['files_failed']} files")
        
        if args.speaker_diarization:
            speakers = result.get('unique_speakers', [])
            total_segments = result.get('total_segments', 0)
            print(f"ğŸ­ Found {len(speakers)} unique speakers: {speakers}")
            print(f"ğŸ“Š Extracted {total_segments} speaker segments")
            print(f"ğŸ’¡ Next: Use 'segment-speakers' to extract character-specific clips")
        
        if result.get('errors'):
            print("Errors:")
            for error in result['errors'][:5]:
                print(f"   - {error}")
    else:
        print(f"âŒ Transcription failed: {result.get('error', 'Unknown error')}")
        return 1
    
    return 0


async def segment_speakers(args):
    """Extract character-specific audio clips from transcribed episodes."""
    print(f"âœ‚ï¸ Segmenting speakers: {args.audio} + {args.transcripts} â†’ {args.output}")
    
    from pipeline.stages.speaker_segmenter import SpeakerSegmenter
    
    segmenter = SpeakerSegmenter(args.config) if args.config else SpeakerSegmenter()
    
    if args.analyze_only:
        # Only analyze speaker patterns
        result = await segmenter.analyze_speakers(args.transcripts)
        
        if result.get('success'):
            speakers = result.get('speaker_analysis', {})
            print(f"ğŸ“Š Speaker Analysis Results:")
            print(f"   Found {result.get('speakers_found', 0)} unique speakers")
            
            for speaker_id, data in speakers.items():
                segments = data.get('total_segments', 0)
                duration = data.get('total_duration', 0)
                episodes = len(data.get('episodes', []))
                
                print(f"\n   Speaker: {speaker_id}")
                print(f"      Segments: {segments}")
                print(f"      Duration: {duration:.1f}s ({duration/60:.1f}min)")
                print(f"      Episodes: {episodes}")
                
                sample_texts = data.get('sample_texts', [])
                if sample_texts:
                    print(f"      Sample text: \"{sample_texts[0][:50]}...\"")
            
            print(f"\nğŸ’¡ Use speaker mapping config to assign character names")
        else:
            print(f"âŒ Speaker analysis failed: {result.get('error', 'Unknown error')}")
            return 1
    
    else:
        # Extract character segments
        result = await segmenter.process_directory(args.audio, args.transcripts, args.output)
        
        if result['success']:
            print(f"âœ… Processed {result['episodes_processed']} episodes successfully")
            print(f"ğŸ“Š Extracted {result['total_segments']} character segments")
            
            character_stats = result.get('character_stats', {})
            if character_stats:
                print(f"\nğŸ­ Character breakdown:")
                for character, stats in character_stats.items():
                    segments = stats.get('segments', 0)
                    duration = stats.get('duration', 0)
                    print(f"   {character}: {segments} segments ({duration/60:.1f} min)")
            
            if result.get('episodes_failed', 0) > 0:
                print(f"âš ï¸ Failed to process {result['episodes_failed']} episodes")
            
            print(f"ğŸ’¡ Next: Use 'preprocess-audio' on character segments for TTS training")
        else:
            print(f"âŒ Speaker segmentation failed: {result.get('error', 'Unknown error')}")
            return 1
    
    return 0


async def analyze_speakers(args):
    """Analyze speaker patterns in transcripts."""
    print(f"ğŸ” Analyzing speakers in: {args.transcripts}")
    
    from pipeline.stages.speaker_segmenter import SpeakerSegmenter
    
    segmenter = SpeakerSegmenter()
    result = await segmenter.analyze_speakers(args.transcripts)
    
    if result.get('success'):
        speakers = result.get('speaker_analysis', {})
        print(f"ğŸ“Š Found {result.get('speakers_found', 0)} unique speakers")
        
        # Sort speakers by total duration
        sorted_speakers = sorted(
            speakers.items(), 
            key=lambda x: x[1].get('total_duration', 0), 
            reverse=True
        )
        
        for speaker_id, data in sorted_speakers:
            segments = data.get('total_segments', 0)
            duration = data.get('total_duration', 0)
            episodes = len(data.get('episodes', []))
            
            print(f"\nğŸ­ Speaker: {speaker_id}")
            print(f"    Segments: {segments}")
            print(f"    Duration: {duration:.1f}s ({duration/60:.1f}min)")
            print(f"    Episodes: {episodes}")
            
            sample_texts = data.get('sample_texts', [])
            if sample_texts:
                print(f"    Sample quotes:")
                for i, text in enumerate(sample_texts[:3]):
                    print(f"      {i+1}. \"{text[:60]}...\"")
        
        # Save detailed report if output specified
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ“„ Detailed report saved to: {args.output}")
        
        print(f"\nğŸ’¡ Use this analysis to update speaker mapping config")
    else:
        print(f"âŒ Speaker analysis failed: {result.get('error', 'Unknown error')}")
        return 1
    
    return 0


async def process_segments(args):
    """Process audio segments to improve quality for TTS training."""
    print(f"ğŸ”§ Processing audio segments: {args.input} â†’ {args.output}")
    
    from pipeline.stages.audio_segment_processor import AudioSegmentProcessor
    
    processor = AudioSegmentProcessor(args.config) if args.config else AudioSegmentProcessor()
    result = await processor.process_segments(args.input, args.output)
    
    if result['success']:
        print(f"âœ… Processing completed!")
        print(f"   ğŸ“Š Speakers processed: {result['speakers_processed']}")
        print(f"   ğŸ¯ Segments accepted: {result['segments_accepted']}/{result['segments_input']}")
        print(f"   ğŸ“‰ Rejection rate: {result['rejection_rate']:.1%}")
        print(f"   â±ï¸ Total duration: {result['total_duration_hours']:.1f} hours")
        
        if result.get('speakers_failed', 0) > 0:
            print(f"   âš ï¸ Failed speakers: {result['speakers_failed']}")
        
        # Show common rejection reasons
        if result['rejection_rate'] > 0.1:  # If >10% rejected
            print(f"\nğŸ’¡ Consider adjusting quality thresholds based on your data")
    else:
        print(f"âŒ Segment processing failed: {result.get('error', 'Unknown error')}")
        return 1
    
    return 0


async def cluster_voices(args):
    """Perform cross-episode voice clustering or apply existing clustering results."""
    from pipeline.stages.voice_clustering import VoiceClusteringSystem
    
    if args.apply:
        # Apply existing clustering results to reorganize segments
        print(f"ğŸ”„ Applying clustering results: {args.apply}")
        print(f"ğŸ“ Reorganizing segments: {args.segments} â†’ {args.output}")
        
        clustering_system = VoiceClusteringSystem()
        result = await clustering_system.apply_clustering_to_segments(
            clustering_results_path=args.apply,
            segments_dir=args.segments,
            output_dir=args.output
        )
        
        if result['success']:
            print(f"âœ… Segments reorganized successfully!")
            print(f"ğŸ“Š Statistics:")
            print(f"   Segments reorganized: {result['segments_reorganized']}")
            print(f"   Characters created: {result['characters_created']}")
            print(f"   Failed segments: {result['segments_failed']}")
            print(f"ğŸ“ Character segments saved to: {result['output_path']}")
        else:
            print(f"âŒ Failed to apply clustering: {result.get('error', 'Unknown error')}")
            return 1
    else:
        # Perform new voice clustering analysis
        print(f"ğŸ¯ Analyzing voices for clustering: {args.segments}")
        print(f"ğŸ“ Clustering results will be saved to: {args.output}")
        
        # Load config if provided
        config = None
        if args.config:
            import yaml
            with open(args.config, 'r') as f:
                config = yaml.safe_load(f)
        
        clustering_system = VoiceClusteringSystem(config)
        result = await clustering_system.cluster_voices(args.segments, args.output)
        
        if result['success']:
            print(f"âœ… Voice clustering completed successfully!")
            print(f"ğŸ“Š Clustering Results:")
            print(f"   Total voice embeddings: {result['total_embeddings']}")
            print(f"   Clusters found: {result['clusters_found']}")
            print(f"ğŸ“ Results saved to: {result['output_path']}")
            
            # Show cluster assignments
            if 'cluster_assignments' in result:
                print(f"ğŸ­ Character Assignments:")
                for cluster_id, character in result['cluster_assignments'].items():
                    print(f"   Cluster {cluster_id} â†’ {character}")
            
            # Show recommendations if available
            if 'recommendations' in result:
                print(f"ğŸ’¡ Recommendations:")
                for rec in result['recommendations'][:3]:  # Show first 3
                    print(f"   â€¢ {rec}")
            
            print(f"\nğŸ”„ To apply these results, run:")
            print(f"   python main.py cluster-voices --segments {args.segments} --apply {args.output}/voice_clustering_results.json --output resources/characters/")
        else:
            print(f"âŒ Voice clustering failed: {result.get('error', 'Unknown error')}")
            return 1
    
    return 0


async def train_model(args):
    """Train a TTS model."""
    print(f"ğŸ‹ï¸ Training {args.model} model with dataset: {args.dataset}")
    
    model_trainer = ModelRegistry.get_trainer(args.model)
    
    # Load config and override with CLI args if provided
    config = model_trainer.load_config(args.config)
    if args.epochs:
        config.training.epochs = args.epochs
    if args.batch_size:
        config.training.batch_size = args.batch_size
    
    result = await model_trainer.train(
        dataset_path=args.dataset,
        output_path=args.output,
        config=config,
        resume_from=args.resume
    )
    
    if result.success:
        print(f"âœ… Training completed!")
        print(f"ğŸ“ Model saved to: {result.model_path}")
        print(f"ğŸ“Š Final metrics: {result.final_metrics}")
    else:
        print(f"âŒ Training failed: {result.error}")
        return 1
    
    return 0


async def run_inference(args):
    """Test model inference."""
    print(f"ğŸ¤ Running inference with model: {args.model}")
    
    model_type = ModelRegistry.detect_model_type(args.model)
    model = ModelRegistry.load_model(model_type, args.model)
    
    result = await model.synthesize(
        text=args.text,
        reference_audio=args.reference,
        output_path=args.output,
        streaming=args.streaming
    )
    
    if result.success:
        print(f"âœ… Audio generated: {args.output}")
        print(f"â±ï¸ Generation time: {result.generation_time:.2f}s")
    else:
        print(f"âŒ Inference failed: {result.error}")
        return 1
    
    return 0


async def launch_discord_bot(args):
    """Launch the Discord bot."""
    print(f"ğŸ¤– Launching Discord bot with {args.model} model")
    
    from discord_bot.bot import TTSBot
    
    bot = TTSBot(
        model_type=args.model,
        voice_clone_path=args.voice_clone
    )
    
    try:
        await bot.run(args.token)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Discord bot stopped")
    except Exception as e:
        print(f"âŒ Bot error: {e}")
        return 1
    
    return 0


async def validate_data(args):
    """Validate audio/dataset quality."""
    print(f"ğŸ” Validating {args.type}: {args.input}")
    
    if args.type == "audio":
        from pipeline.validators.audio_quality import AudioQualityValidator
        validator = AudioQualityValidator()
    elif args.type == "dataset":
        from pipeline.validators.dataset_validator import DatasetValidator
        validator = DatasetValidator()
    else:
        from pipeline.validators.transcript_alignment import TranscriptAlignmentValidator
        validator = TranscriptAlignmentValidator()
    
    result = await validator.validate_directory(args.input)
    
    print(f"âœ… Validation completed:")
    print(f"   ğŸ“Š Quality score: {result.overall_score:.2f}/10")
    print(f"   âœ… Passed: {result.passed_files}")
    print(f"   âŒ Failed: {result.failed_files}")
    
    if result.issues:
        print("\nâš ï¸ Issues found:")
        for issue in result.issues:
            print(f"   - {issue}")
    
    return 0 if result.overall_score >= 7.0 else 1


async def create_validation_samples(args):
    """Create validation samples for manual review of clustering quality."""
    # Load clustering results
    with open(args.clustering_results, 'r') as f:
        data = json.load(f)
    
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Clear existing validation samples
    for item in output_dir.iterdir():
        if item.is_dir():
            shutil.rmtree(item)
        else:
            item.unlink()
    
    print(f"ğŸ§ Creating validation samples in: {output_dir}")
    
    # Process each cluster
    for cluster in data['clusters']:
        cluster_id = cluster['cluster_id']
        character = cluster.get('assigned_character', f'cluster_{cluster_id}')
        duration = cluster['total_duration'] / 60
        episodes = len(cluster['episodes'])
        
        # Create character directory
        char_dir = output_dir / f"{cluster_id:02d}_{character}"
        char_dir.mkdir(exist_ok=True)
        
        # Copy representative samples
        samples = cluster.get('representative_samples', [])[:args.samples_per_cluster]
        
        print(f"ğŸ“ {character}: {duration:.1f}min, {episodes} episodes, copying {len(samples)} samples")
        
        for i, sample_path in enumerate(samples):
            if Path(sample_path).exists():
                # Create descriptive filename
                original_name = Path(sample_path).name
                new_name = f"{i+1:02d}_{original_name}"
                dest_path = char_dir / new_name
                
                try:
                    shutil.copy2(sample_path, dest_path)
                except Exception as e:
                    print(f"  âš ï¸  Failed to copy {sample_path}: {e}")
            else:
                print(f"  âš ï¸  Sample not found: {sample_path}")
        
        # Create a summary file for this cluster
        summary_file = char_dir / "cluster_info.txt"
        with open(summary_file, 'w') as f:
            f.write(f"Cluster ID: {cluster_id}\n")
            f.write(f"Character: {character}\n")
            f.write(f"Duration: {duration:.1f} minutes\n")
            f.write(f"Episodes: {episodes}\n")
            f.write(f"Segments: {cluster['segment_count']}\n")
            f.write(f"Quality Score: {cluster.get('quality_score', 'N/A')}\n")
            f.write(f"\nFirst few episodes:\n")
            for ep in cluster['episodes'][:5]:
                f.write(f"  - {ep}\n")
    
    print(f"\nâœ… Validation samples created in: {output_dir}")
    print("ğŸ“ Each cluster has a 'cluster_info.txt' file with details")
    print("ğŸµ Audio files are numbered for easy sequential listening")


async def main():
    """Main entry point."""
    parser = setup_parser()
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return 1
    
    # Setup logging
    setup_logging(verbose=args.verbose)
    
    # Route to appropriate handler
    handlers = {
        "run-pipeline": run_pipeline,
        "extract-audio": extract_audio,
        "preprocess-audio": preprocess_audio,
        "transcribe": transcribe_audio,
        "segment-speakers": segment_speakers,
        "analyze-speakers": analyze_speakers,
        "process-segments": process_segments,
        "cluster-voices": cluster_voices,
        "train": train_model,
        "inference": run_inference,
        "discord-bot": launch_discord_bot,
        "validate": validate_data,
        "create-validation-samples": create_validation_samples,
    }
    
    handler = handlers.get(args.command)
    if not handler:
        print(f"âŒ Unknown command: {args.command}")
        return 1
    
    try:
        return await handler(args)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Operation cancelled by user")
        return 1
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code) 