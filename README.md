# TTS Trainer - Pipeline-Orchestrated Architecture

A comprehensive toolkit for converting video files into training data and fine-tuning Text-to-Speech models with real-time Discord bot integration.

## ğŸ¯ Features

- **Video-to-TTS Pipeline**: Automated conversion from video files to TTS training datasets
- **Multiple Model Support**: XTTS v2, VITS, and Tortoise-TTS with unified interface
- **Audio Processing**: Professional-grade audio preprocessing with denoising and normalization
- **Real-time Transcription**: Faster-Whisper integration with speaker diarization
- **Discord Bot**: Stream TTS output directly to Discord voice channels
- **Experiment Tracking**: Built-in metrics and checkpointing system

## ğŸ—ï¸ Architecture

This project follows a **Pipeline-Orchestrated Architecture** with clear data flow stages:

```
Video Files â†’ Audio Extraction â†’ Preprocessing â†’ Transcription â†’ Dataset Building â†’ Model Training â†’ Deployment
```

### Key Components

- **Pipeline Orchestrator**: Manages the entire workflow from video to trained model
- **Stage-based Processing**: Each step is isolated and can be run independently
- **Validation Checkpoints**: Quality assurance at each stage
- **Model Adapters**: Unified interface for different TTS architectures

## ğŸ“ Project Structure

```
tts-trainer/
â”œâ”€â”€ config/                 # Configuration files
â”‚   â”œâ”€â”€ models/            # Model-specific configs (XTTS, VITS, Tortoise)
â”‚   â”œâ”€â”€ audio/             # Audio processing settings
â”‚   â””â”€â”€ training/          # Training hyperparameters
â”œâ”€â”€ docs/                   # Documentation
â”‚   â”œâ”€â”€ implementation_roadmap.md  # Feature implementation plan
â”‚   â””â”€â”€ deep_research_plan.md      # Research and architecture notes
â”œâ”€â”€ resources/             # Data storage
â”‚   â”œâ”€â”€ videos/           # Input video files
â”‚   â”œâ”€â”€ audio/            # Extracted/processed audio
â”‚   â”œâ”€â”€ transcripts/      # Generated transcriptions
â”‚   â””â”€â”€ datasets/         # Final training datasets
â”œâ”€â”€ src/                   # Source code
â”‚   â”œâ”€â”€ pipeline/         # Pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ stages/       # Individual processing stages
â”‚   â”‚   â””â”€â”€ validators/   # Quality validation
â”‚   â”œâ”€â”€ models/           # Model implementations
â”‚   â”‚   â”œâ”€â”€ xtts/        # XTTS v2 implementation
â”‚   â”‚   â””â”€â”€ vits/        # VITS implementation
â”‚   â”œâ”€â”€ utils/           # Utility functions
â”‚   â””â”€â”€ discord_bot/     # Discord integration
â”œâ”€â”€ notebooks/           # Jupyter notebooks for analysis
â”œâ”€â”€ tests/              # Test suite
â”œâ”€â”€ artifacts/          # Training outputs
â”‚   â”œâ”€â”€ models/        # Trained models
â”‚   â”œâ”€â”€ checkpoints/   # Training checkpoints
â”‚   â””â”€â”€ metrics/       # Training metrics
â””â”€â”€ main.py            # CLI entry point
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- NVIDIA RTX 4090 (or equivalent)
- CUDA 11.8+
- 16GB+ VRAM recommended

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd tts-trainer

# Install dependencies
pip install -r requirements.txt

# Verify GPU setup
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

### Basic Usage

1. **Prepare your data**: Place video files in `resources/videos/`

2. **Run the full pipeline**:
```bash
python main.py run-pipeline --input resources/videos/ --output artifacts/models/
```

3. **Train a model**:
```bash
python main.py train --model xtts_v2 --dataset resources/datasets/my_voice/
```

4. **Test inference**:
```bash
python main.py inference --model artifacts/models/my_model.pth --text "Hello world!"
```

5. **Launch Discord bot**:
```bash
python main.py discord-bot --token YOUR_DISCORD_TOKEN
```

## ğŸ”§ Configuration

### Model Configuration

Edit `config/models/xtts_v2.yaml` for XTTS v2 settings:

```yaml
model:
  name: "xtts_v2"
  device: "cuda"
  precision: "fp16"
  
streaming:
  chunk_size: 2048
  overlap: 256
  
training:
  batch_size: 8
  learning_rate: 1e-4
  epochs: 10
```

### Audio Processing

Configure audio preprocessing in `config/audio/preprocessing.yaml`:

```yaml
input:
  sample_rate: 48000
  format: "wav"
  
preprocessing:
  denoise: true
  normalize_lufs: -23
  trim_silence: true
  
output:
  sample_rate: 24000
  bit_depth: 16
```

## ğŸ® Discord Bot Integration

The Discord bot provides real-time TTS streaming:

### Features
- Voice cloning from 6+ seconds of audio
- Real-time speech-to-text transcription
- Speaker identification and separation
- Streaming audio output (200ms latency)

### Commands
- `!say <text>` - Speak text in the voice channel
- `!clone <audio_file>` - Clone voice from audio file
- `!listen` - Start transcribing voice channel
- `!stop` - Stop current operations

## ğŸ“Š Supported Models

| Model | Training Time | Quality | Real-time Capable | Use Case |
|-------|---------------|---------|-------------------|----------|
| **XTTS v2** | No training needed | High | âœ… | Quick voice cloning |
| **VITS** | 30 min on 4090 | Very High | âœ… | Fine-tuned quality |
| **Tortoise-TTS** | 2-4 hours | Ultra High | âŒ | Studio-quality offline |

## ğŸ”¬ Development

### Running Tests
```bash
# Run all tests
pytest tests/

# Run specific test category
pytest tests/unit/
pytest tests/integration/
```

### Adding New Models
1. Create model adapter in `src/models/{model_name}/`
2. Implement the base trainer interface
3. Add configuration in `config/models/{model_name}.yaml`
4. Register in the pipeline orchestrator

### Pipeline Stages
Each stage implements the `PipelineStage` interface:
- `validate_input()` - Check input requirements
- `process()` - Execute the stage logic
- `validate_output()` - Verify output quality

## ğŸ“ˆ Performance Benchmarks

**RTX 4090 Performance:**
- XTTS v2 Inference: 200ms time-to-first-chunk
- Faster-Whisper Transcription: Real-time with VAD
- VITS Training: 30 minutes for 10-minute dataset
- Peak VRAM Usage: ~7GB (STT + TTS simultaneous)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- [Coqui TTS](https://github.com/coqui-ai/TTS) for XTTS v2 and VITS
- [Faster-Whisper](https://github.com/SYSTRAN/faster-whisper) for transcription
- [pyannote.audio](https://github.com/pyannote/pyannote-audio) for speaker diarization
- [discord.py](https://github.com/Rapptz/discord.py) for Discord integration 